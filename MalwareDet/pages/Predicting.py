# ===================== Import Packages ===========================

import streamlit as st
import pandas as pd
import os
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
import base64

# ================ Background image ===

def add_bg_from_local(image_file):
    with open(image_file, "rb") as image_file:
        encoded_string = base64.b64encode(image_file.read())
    st.markdown(
    f"""
    <style>
    .stApp {{
        background-image: url(data:image/{"png"};base64,{encoded_string.decode()});
        background-size: cover
    }}
    </style>
    """,
    unsafe_allow_html=True
    )
add_bg_from_local('2.jpg')   


# ================  INPUT  ===

st.title(" Malware Detection Using ML ")

#============================= DATA SELECTION ==============================

dataframe=pd.read_csv("Malware dataset.csv")
print("----------------------------")
print("DATA SELECTION")
print("----------------------------")
print(dataframe.head(20))

#============================= PREPROCESSING ==============================

#==== checking missing values ====

print("----------------------------")
print("CHECKING MISSING VALUES")
print("----------------------------")
print(dataframe.isnull().sum())


#==== label encoding ====
print("----------------------------")
print("BEFORE LABEL ENCODING")
print("----------------------------")
print()
print(dataframe['classification'].head(15))
print()
label_encoder=preprocessing.LabelEncoder()

print("----------------------------")
print("AFTER LABEL ENCODING")
print("----------------------------")
print()
dataframe['classification']=label_encoder.fit_transform(dataframe['classification'])
print(dataframe['classification'].head(15))
print()

#============================= PCA ==============================


# X = dataframe.drop(["hash","classification",'vm_truncate_count','shared_vm','exec_vm','nvcsw','maj_flt','utime'],axis=1)

X = dataframe.drop(["hash","classification",'vm_truncate_count','shared_vm','exec_vm','nvcsw','maj_flt','utime'],axis=1)



Y = dataframe["classification"]


from sklearn.decomposition import PCA

pca = PCA(n_components = 20)
 
X_train = pca.fit_transform(X)


print("---------------------------------------------------")
print("PRINCIPLE COMPONET ANALYSIS")
print("---------------------------------------------------")
print()
print(" The original features is :", X.shape[1])
print()
print(" The reduced feature is :",X_train.shape[1 ] )
print()


# ======================== EDA ======================================


import matplotlib.pyplot as plt
plt.hist(Y)
plt.show() 


import seaborn as sns
plt.figure(figsize=(5, 5))
plt.title("Classification")
sns.countplot(x='classification',data=dataframe)
plt.show()



import seaborn as sns
plt.figure(figsize=(5, 5))
plt.title("USAGE COUNTER")
sns.countplot(x='usage_counter',data=dataframe)
plt.show()


import seaborn as sns
plt.figure(figsize=(5, 5))
plt.title("STIME")
sns.countplot(x='stime',data=dataframe)
plt.show()


import seaborn as sns
plt.figure(figsize=(5, 5))
plt.title("GTIME")
sns.countplot(x='gtime',data=dataframe)
plt.show()


#======================== DATA SPLITTING ================================

from matplotlib import pyplot as plt 
from sklearn import metrics

from sklearn.model_selection import train_test_split

print("----------------------------------------")
print("DATA SPLITTING")
print("------------------------------------")
print()
x_train,x_test,y_train,y_test=train_test_split(X,Y,test_size=0.3,random_state=1)


print("1. Total Number of Data       = ", len(X))
print("2. Total Number of Test Data  = ", len(x_test))
print("3. Total Number of Train Data = ", len(x_train))


#======================== CLASSIFICATION ==============================

# ======== MLP ==========


from sklearn.neural_network import MLPClassifier

regressor = MLPClassifier() 
 
# fit the regressor with x and y data
regressor.fit(x_train, y_train) 

Y_pred_mlp = regressor.predict(x_train)

mae_mlp = metrics.mean_absolute_error(y_train,Y_pred_mlp)

acc_mlp = 100 - mae_mlp


print("-------------------------------------")
print("   Multi Layer Perceptron ---> MLP   ")
print("-------------------------------------")
print()
print("1. Accuracy = ",acc_mlp, '%')
print()
print(" 2. Error Rate = ",mae_mlp )


# ====LINEAR REGRESSION ===

from sklearn.linear_model import LinearRegression

model = LinearRegression()

# Model training
model.fit(x_train, y_train)

Y_pred = model.predict(x_train)

from sklearn import metrics

mae_lr = metrics.mean_absolute_error(y_train,Y_pred)

acc_lr = 100 - mae_lr


print("--------------------------------")
print("   LOGISTIC REGRESSION ---> LR  ")
print("--------------------------------")
print()
print("1. Accuracy = ",acc_lr, '%')
print()
print(" 2. Error Rate = ",mae_mlp )



#============  COMPARISON GRAPH =============

import matplotlib.pyplot as plt 
vals=[acc_mlp,acc_lr]
inds=range(len(vals))
labels=["MLP","LR"]
fig,ax = plt.subplots()
rects = ax.bar(inds, vals)
ax.set_xticks([ind for ind in inds])
ax.set_xticklabels(labels)
plt.title("MLP vs LR")
plt.savefig('graph.png')
plt.show()



# === STORE MODEL ====

import pickle

filename = 'malware.pkl'
pickle.dump(regressor, open(filename, 'wb'))


# ======================= PREDICTION =====================================

st.markdown(f'<h1 style="color:#FFFFFF;font-size:24px;">{" Kindly enter the below details for identifing the malware or not"}</h1>', unsafe_allow_html=True)


#  1

milli=st.text_input("Enter Millisecond",'0')
milli=int(milli)


#  2

state=st.text_input("Enter State of the task",'0')
state=int(state)


#  3

usage=st.text_input("Enter Usage Counter",'0')
usage=int(usage)


#  4

prio=st.text_input("Enter Pririty Queue",'0')
prio=int(prio)


#  5

staprio=st.text_input("Enter Static Pririty Queue",'0')
staprio=int(staprio)


#  6

norprio=st.text_input("Enter Normal Pririty Queue",'0')
norprio=int(norprio)

#  7

policy=st.text_input("Enter Task planning policy ",'0')
policy=int(policy)


#  8

vm_pgoff=st.text_input("Enter Offset of the area in the file",'0')
vm_pgoff=int(vm_pgoff)


#  9

task_size=st.text_input("Enter Current task size ",'0')
task_size=int(task_size)


#  10

cached_hole_size=st.text_input("Enter Free address space hole size",'0')
cached_hole_size=int(cached_hole_size)


#  11

free_area_cache=st.text_input("Enter First address of space hole ",'0')
free_area_cache=int(free_area_cache)


#  12

mm_users=st.text_input("Enter Space users ",'0')
mm_users=int(mm_users)


#  13

map_count=st.text_input("Enter Count of memory areas",'0')
map_count=int(map_count)



#  14

hiwater_rss=st.text_input("Enter Peak of resident set size  ",'0')
hiwater_rss=int(hiwater_rss)



#  15

total_vm=st.text_input("Enter Total number of pages ",'0')
total_vm=int(total_vm)


#  16

reserved_vm=st.text_input("Enter Reserved pages count ",'0')
reserved_vm=int(reserved_vm)


#  17

nr_ptes=st.text_input("Enter Page table entries count ",'0')
nr_ptes=int(nr_ptes)


#  18

end_data=st.text_input("Enter End address of code component ",'0')
end_data=int(end_data)


#  19

last_interval=st.text_input("Enter Last interval time before thrashing ",'0')
last_interval=int(last_interval)



#  20

nivcsw=st.text_input("Enter in-volunteer context switches count ",'0')
nivcsw=int(nivcsw)


#  21

min_flt=st.text_input("Enter Minor faults (page faults) ",'0')
min_flt=int(min_flt)


#  22

fs_excl_counter=st.text_input("Enter Count of file system exclusive resources )",'0')
fs_excl_counter=int(fs_excl_counter)


#  23

lock=st.text_input("Enter Read-write synchronization lock",'0')
lock=int(lock)


#  24

stime=st.text_input("Enter System time ",'0')
stime=int(stime)


#  25

gtime=st.text_input("Enter Guest time",'0')
gtime=int(gtime)



#  26

cgtime=st.text_input("Enter Cumulative group time",'0')
cgtime=int(cgtime)



#  27

signal_nvcsw=st.text_input("Enter Cumulative resource counter",'0')
signal_nvcsw=int(signal_nvcsw)


import numpy as np
input_1 = np.array([milli,state,usage,prio,staprio,norprio,policy,vm_pgoff,task_size,cached_hole_size,free_area_cache,mm_users,map_count,hiwater_rss,total_vm,reserved_vm,nr_ptes,end_data,last_interval,nivcsw,min_flt,min_flt,lock,stime,gtime,cgtime,signal_nvcsw]).reshape(1, -1)

predicted_data = regressor.predict(input_1)


aa=st.button("PREDICT")

if aa:

    if predicted_data==1:
        st.markdown(f'<h1 style="color:#006400;font-size:14px;">{"MALWARE DETECTED"}</h1>', unsafe_allow_html=True)
        from faker import Faker
        ex = Faker()
        ip_rec = ex.ipv4()
        ip_sen = ex.ipv4()
        
        
        st.write("Sender's IP Address   = ",ip_sen )
        st.write("Reciever's IP Address = ",ip_rec )


    
    else:
        st.markdown(f'<h1 style="color:#006400;font-size:14px;">{"NO MALWARE"}</h1>', unsafe_allow_html=True)





# from random import randint


# def generate_random_ip():
#     return '.'.join(
#         str(randint(0, 255)) for _ in range(4)
#     )


# random_ip = generate_random_ip()
# print(random_ip)  # üëâÔ∏è 178.166.111.80

# random_ip = generate_random_ip()
# print(random_ip)  




















